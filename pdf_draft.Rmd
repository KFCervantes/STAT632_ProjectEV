---
title: "pdf_draft"
author: "Kaleb Cervantes"
date: "4/20/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Model Selection
## Variable Selection
The first thing was to check the correlations between some of the variables and visualize the data.

```{r, echo=FALSE, message=FALSE}
library(dplyr)
library(knitr)
library(ggplot2)
library(magrittr)
library(tidyr)
data <- read.csv("Dataset/cleaned_data.csv") %>%
  drop_na

numeric_data <- data %>%
  select_if(is.numeric) %>%
  select(-"PriceinGermany")

numeric_data %>% cor

pairs(
  PriceinUK ~ .,
  numeric_data
)
```

The most highly correlated variables are `Subtitle` and `Range`. These have a correlation of 0.91104205 and seem to have similar plots in the scatter plot matrix. A zoomed in scatter plot is included below.

```{r, echo=FALSE}
ggplot(data, aes(Subtitle, Range)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x, se = F)
```

When zoomed in, the two variables seem to have a linear relationship with each other. This indicates that one of these may be dropped. `Subtitle` and `Range` have respective correlations 0.7039080 and 0.6843999 with the response. Since `Subtitle` has a stronger correlation with the response, that will be the predictor that is kept.

## Transformations
In the first row of the scatter plot matrix, `TopSpeed` and `Acceleration` appear to form a parabola when next to each other. As such, a quadratic transformation will be applied to those two variables.

`NumberofSeats` also appears to behave as a factor in the scatter plot matrix. As such, it will be transformed into one.

In order to handle non-constant variance, logarithmic transformations will be applied to the other predictors and response variables.

## Full Model
```{r, echo = FALSE, warning=FALSE}
full_model <- (
  1 / sqrt(PriceinUK) ~
    log(Range) +
    poly(Acceleration, 2, raw = T) +
    poly(TopSpeed, 2, raw = T) +
    poly(Efficiency, 2, raw = T) +
    log(FastChargeSpeed) +
    Drive +
    NumberofSeats
) %>%
  lm(data)

full_model %>%
  summary

full_std_residuals <- full_model %>%
  rstandard

ggplot(
  mapping = aes(
    x = full_model$fitted.values,
    y = full_model$residuals,
    title
  )
) +
  geom_point(aes(color = full_std_residuals %>% abs > 2)) +
  geom_smooth(se = F) +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    color = "High Standardized\nResidual\n(absolute value > 2)",
    title = "Residuals vs Fitted Values"
  )

ggplot(mapping = aes(sample = full_std_residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(
    x = "Theorhetical Quantiles",
    y = "Standardized Residuals",
    title = "Normal Q-Q"
  )
```

## Reduced Model
This full model seems to have met the assumptions for linear regression. However, it still contains predictors that are not significant. In order to reduce the model, a step-wise process will be used with BIC as the metric.

```{r, echo=FALSE}
reduced_model <- full_model %>%
  step(
    trace = 0,
    k = data %>% nrow %>% log
  )

reduced_model %>%
  summary
```

This process removed `log(FastChargeSpeed)` and `Drive` from the model. The assumptions still appear to be met. In order to ensure no significant predictors were removed, a partial $F$-Test will be conducted between the two models.

```{r, echo=FALSE}
anova(reduced_model, full_model)
```

Since the $p$-value is high, we lack statistically significant evidence that any significant predictors were dropped.

This leaves us with the formula:

\begin{align*}
\ln \texttt{PriceinUK} &= \beta_0 \\
&+ \beta_1 \ln \texttt{Subtitle} \\
&+ \beta_2 \texttt{Acceleration} + \beta_3 \texttt{Acceleration}^2 \\
&+ \beta_4 \texttt{TopSpeed} + \beta_5 \texttt{TopSpeed}^2 \\
&+ \beta_6 \ln \texttt{Efficiency} \\
&+ \beta_7 \left( \texttt{NumberofSeats} == 5 \right) + \beta_8 \left( \texttt{NumberofSeats} == 7 \right) \\
&+ \epsilon
\end{align*}

It may also help to check some diagnostic plots.

```{r, echo=FALSE}
reduced_std_residuals <- reduced_model %>%
  rstandard

ggplot(
  mapping = aes(
    x = reduced_model$fitted.values,
    y = reduced_model$residuals,
    title
  )
) +
  geom_point(aes(color = reduced_std_residuals %>% abs > 2)) +
  geom_smooth(se = F) +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    color = "High Standardized\nResidual\n(absolute value > 2)",
    title = "Residuals vs Fitted Values"
  )

ggplot(mapping = aes(sample = reduced_std_residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(
    x = "Theorhetical Quantiles",
    y = "Standardized Residuals",
    title = "Normal Q-Q"
  )
```

From the visual tests, the normality and constant variance conditions appear to be met. However it may help to perform some tests.

First, a Shapiro-Wilkes test will be used to test for normality.

```{r, echo=FALSE}
reduced_std_residuals %>% shapiro.test
```

Here, the $p$-value is significant to reject normality. However, the $W$-statistic is larger than 0.95, which indicates that the standardized residuals are highly correlated with the theoretical quantiles. As such, there is not enough evidence to refute normality. Since normality is not rejected, we now test for constant variance.

```{r, echo=FALSE}
reduced_model %>% (lmtest::bptest)
```

Since the $p$-value is low, we reject constant variance.

Using the data, we can determine some of the high leverage points:

```{r, echo=FALSE}
reduced_leverage <- reduced_model %>% hatvalues
data %>%
  subset(
    abs(reduced_leverage) > 2 * mean(reduced_leverage),
    c("Name", "PriceinUK")
  ) %>%
  kable
```

and the high residual points

```{r, echo=FALSE}
data %>%
  subset(
    abs(reduced_std_residuals) > 2,
    c("Name", "PriceinUK")
  ) %>%
  kable
```

In both of these sets, a significant fraction of the cars are Teslas. For the high residual points, a decent amount were also Mercedes.
