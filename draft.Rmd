---
title: "Draft"
output: github_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(car)
library(dplyr)
library(ggplot2)
library(knitr)
library(magrittr)
library(tidyr)
data <- "Dataset/cleaned_data.csv" %>%
  read.csv
```

This document is meant to draft out stuff and select variables.

# Correlation Matrix

First, variables will need to be selected. In order to do this, a correlation matrix will be used to pick which variables will be kept before doing linear regression. For our purposes, we will define highly correlated as having an absolute correlation higher than 0.7.

```{r}
data %>%
  select_if(is.numeric) %>%
  cor(use = "pairwise.complete.obs") %>%
  kable
```
From this matrix, we can see that `PriceinUK` is highly correlated with the following:

* `Subtitle`

* `TopSpeed`

* `PriceinGermany`

`PriceinGermany` may be considered a different version of the response, but `Subtitle` and `TopSpeed` may be significant predictors. For this reason, other predictors correlated with these may be dropped. These predictors are:

* `Range`

* `Acceleration`

* `FastChargeSpeed`

# Scatterplot Matrix

Next a scatterplot matrix will be used to find potential transformations.

```{r}
(PriceinUK ~ .) %>%
  pairs(
    data %>%
      select_if(is.numeric) %>%
      select(-c("PriceinGermany", "Range", "Acceleration", "FastChargeSpeed"))
  )
```
In order to find the appropriate transformations, the first row will be focused on. This is because it contains `PriceinUK` as the response and the other variables as a single predictor.

Since `Subtitle` and `TopSpeed` seem to curve upwards, a quadratic transformation will be used for those.

Since `Efficiency` and `NumberofSeats` seem to have a non-constant variance, a logistic transformation will be used. Since `NumberofSeats` is always positive, the transformation `log` will work. In order to find which logistic transformation, we will have check if `Efficiency` has non-positive values.

```{r}
data %$%
  Efficiency %>%
  min
```

Since the minimum value in `Efficiency` is positive, all values in that column must be positive. This indicates that the transformation `log` will work.

# First Model

Now a model will be tested. Initially, a full model will be used and the `step` function will be used to select significant predictors using the BIC.

Before we test the model, it may help to subset the data in order to drop rows with null values for the columns we want.

```{r}
lm_data <- data %>%
  select(-c("Acceleration", "Range", "FastChargeSpeed", "PriceinGermany")) %>%
  drop_na()
```

Now that we have the desired subset, we can use it for regression.

```{r}
lm1 <- (PriceinUK ~ poly(Subtitle, 2, raw = T) + poly(TopSpeed, 2, raw = T) + log(Efficiency) + log(NumberofSeats) + Drive) %>%
  lm(lm_data) %>%
  step(
    trace = 0,
    k = lm_data %>% nrow %>% log
  )

lm1 %>%
  summary

lm1 %>%
  plot(which = 1:2)
```

In this model, the term `TopSpeed` does not seem to be significant. As such, a new model without this term will be fit and analyzed. Since the column `Drive` does not contain null values, we can reuse the same dataset.

```{r}
lm2 <- (PriceinUK ~ poly(Subtitle, 2, raw = T) + I(TopSpeed^2) + log(Efficiency) + log(NumberofSeats)) %>%
  lm(lm_data) %>%
  step(
    trace = 0,
    k = lm_data %>% nrow %>% log
  )

lm2 %>%
  summary

lm2 %>%
  plot(which = 1:2)
```

This new model only contains significant predictors, however a response transformation may be needed. In order to check this, the `powerTransform` function will be used.

```{r}
lm2 %>%
  powerTransform %>%
  summary
```

This indicates that an inverse square root transformation may be appropriate. A new version of the model with this transformation will be used and analyzed.

```{r}
lm3 <- (
  (1 / sqrt(PriceinUK)) ~
    poly(Subtitle, 2, raw = T) +
    I(TopSpeed^2) +
    log(Efficiency) +
    log(NumberofSeats)
  ) %>%
  lm(lm_data) %>%
  step(
    trace = 0,
    k = lm_data %>% nrow %>% log
  )

lm3 %>%
  summary

lm3 %>%
  plot(which = 1:2)
```

This process has resulted in the following model:

$$
\begin{align*}
\frac{1}{\sqrt{\texttt{PriceinUK}}} = & \beta_0\\
+& \beta_1 \texttt{Subtitle} \\
+& \beta_2 \texttt{Subtitle}^2 \\
+& \beta_3 \texttt{TopSpeed}^2 \\
+& \beta_4 \ln \texttt{Efficiency} \\
+& \beta_5 \ln \texttt{NumberofSeats} \\
+& \epsilon
\end{align*}
$$

# Alternative Model
It is important to note that there is an alternative response in the dataset: `PriceinGermany`. The same process will be repeated as with `PriceinUK`.

## Scatterplot Matrix
```{r}
(PriceinGermany ~ .) %>%
  pairs(
    data %>%
      select_if(is.numeric) %>%
      select(-c("PriceinUK", "Range", "Acceleration", "FastChargeSpeed"))
  )
```

Apart from `NumberofSeats`, there seems to be a positive linear relationship with the response and `Subtitle` and `TopSpeed`. `PriceinGermany` and `Efficiency` seems to have a positive relationship, but not a constant variance.

## Alternative Reduced Model
In order to get the desired subset, we will drop the null values from the desired columns.

```{r}
lm_data2 <- data %>%
  select(-c("Acceleration", "Range", "FastChargeSpeed", "PriceinUK")) %>%
  drop_na()
```

Now the full alternative model will be fit.

```{r}
lm4 <- (PriceinGermany ~ Subtitle + TopSpeed + Efficiency + NumberofSeats + Drive) %>%
  lm(lm_data2) %>%
  step(
    trace = 0,
    k = lm_data2 %>% nrow %>% log
  )

lm4 %>%
  summary

lm4 %>%
  plot(which = 1:2)
```

Since the Residuals vs Fitted Values plot seems to fan out, there also does not seem to be a constant variance. This indicates that not all of the assumptions are met.

## Possible Transformations
We now use box-cox to find a transformation of the response.

```{r}
lm4 %>%
  powerTransform %>%
  summary
```

This seems to indicate that an inverse cube root transformation may help.

# Old Model
For this, I decided to use and absolute correlation of 0.8 as the cutoff.

```{r}
data %>%
  select_if(is.numeric) %>%
  cor(use = "pairwise.complete.obs")
```

Before dropping predictors, It may also help to visualize them to see if they behave similarly.

```{r}
(PriceinUK ~ .) %>%
  pairs(
    data %>%
      select_if(is.numeric) %>%
      select(-"PriceinGermany")
  )
```

Since `Subtitle` and `Range` yield similar plots, and are highly correlated, one of those should be dropped. Since `Subtitle` has a stronger correlation with the response than `Range`, `Range` will be dropped.

`Accelaration` and `TopSpeed` are also highly correlated. However, it is hard to visualize if the plots are similar since the correlation is negative.

```{r}
data %>%
  summary
```

A full model will be fitted first. This is to test the sigificance of predictors that will be dropped later.

```{r}
lm_data <- data %>%
  drop_na

full_model <- (
  log(PriceinUK) ~
    log(Subtitle) +
    poly(Acceleration, 2, raw = T) +
    poly(TopSpeed, 2, raw = T) +
    log(Efficiency) +
    log(FastChargeSpeed) +
    factor(NumberofSeats) +
    Drive
) %>%
  lm(lm_data)

full_model %>%
  summary
```



```{r}
reduced_model <- full_model %>%
  step(
    trace = 0,
    k = lm_data %>% nrow %>% log
  )
reduced_model %>% summary
```

Now a partial $F$-test will be conducted to see if any of the dropped predictors may be significant.

```{r}
anova(reduced_model, full_model)
```

```{r}
reduced_model %>%
  plot(which = 1:2)
```

```{r}
lm5 <- (
  log(PriceinUK) ~
    log(Subtitle) +
    poly(Acceleration, 2, raw = T) +
    log(Efficiency) +
    log(FastChargeSpeed) +
    NumberofSeats
) %>%
  lm(data)
lm5 %>%
  summary
lm5 %>%
  plot(which = 1:2)
```

# Final Model
This will now be the final model

```{r}
final_model <- (
    log(PriceinUK) ~
      log(Subtitle) +
      poly(Acceleration, 2, raw = T) + 
      poly(TopSpeed, 2, raw = T) +
      log(Efficiency) +
      factor(NumberofSeats)
) %>%
  lm(lm_data)

final_model %>%
  summary

final_model %>%
  plot(which = 1:2)
```