---
title: "Final Paper"
author: "Kotomi Oda, Kaleb Cervantes, Nikhil Taringonda"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
# sets up default settings for code chunks
knitr::opts_chunk$set(
  echo = F,
  message = F,
  warning = F
)
```

```{r loading}

# loads necessary libraries
library(tidyverse)

# load cleaned dataset and rename subtitle column
full_data <- read_csv("Dataset/cleaned_data.csv") %>%
  mutate(battery_capacity = Subtitle, .keep = "unused")

# drop incomplete observations
data <- drop_na(full_data)
```

<!-- Research Question -->
<!-- Incorporate into introduction? -->
Which characteristics of Electric Vehicles have a significant impact on increasing their price?

<!-- Mention data cleaning and why a large portion of the data was removed? -->
<!-- Description of data? -->

```{r matrix_plots, fig.cap="Correlation/Scatterplot/Density Matrix"}

# Plots figure 1
select(data, -Name) %>%
  GGally::ggpairs()
```
<!-- Methods and Results -->
<!-- Descriptive and inferential statistics -->

<!-- Model Selection Process -->
In order to select the model, we began with a full --- and untransformed --- additive model. The purpose of this were to recognize which variables introduced a lot of multicolinearity. From this, the scatterplot matrix, and the correlation matrix, we were able to recognize two pairs of variables that had a lot of multicolinearity: `batery_capacity` and `Range`, `Acceleration` and `TopSpeed`.
<!-- Mention something about VIF with those here -->
We ended up dropping the variables `battery_capacity` and `Acceleration`. This was because these predictors were less accurate in later parts of the process than `Range` and `TopSpeed` respectively.

After this, there were still possible transformations needed for the predictors. Initial predictions were found by looking at marginal plots between the predictors and the response. Other transformations of the predictors would be tested and the more accurate predictions would be kept.

There were also possible transformations needed for the response variable. This was done by using a Box-Cox Power Transformation. In this case, we got $\lambda \approx -0.5$, which corresponds to an inverse square root transformation.

```{r selction}

# stepwise selection w/ BIC
final_model <- lm(
  1 / sqrt(PriceinUK) ~
    log(Range) +
    poly(TopSpeed, 2, raw = T) +
    poly(Efficiency, 2, raw = T) +
    FastChargeSpeed +
    NumberofSeats +
    Drive,
  data
) %>%
  step(trace = 0, k = nrow(data) %>% log)
```

After this, there were still some insignificant predictors remaining. In order to choose the significant ones, stepwise selection --- with BIC as the metric --- was utilized. This resulted in the final model:

<!-- This will render correctly if knitted to pdf -->
\begin{align*}
\frac{1}{\sqrt{\texttt{PriceinUK}}} &= \beta_0 \\
&+ \beta_1 \ln \texttt{Range} \\
&+ \beta_2 \texttt{TopSpeed} \\
&+ \beta_3 \texttt{TopSpeed}^2 \\
&+ \beta_4 \texttt{Efficiency} \\
&+ \beta_5 \texttt{Efficiency}^2 \\
&+ \epsilon
\end{align*}

A summary of the table can be seen below:

<!-- regression summary table -->
<!-- im having trouble adding figure caption to this -->
```{r final_mod_sum, fig.cap="Final Model Summary"}

# prints model summary in table format
summary(final_model) %>%
  pander::pander()
```

Using this model, we decided to recognize the observations that were both high residual and high leverage. These points were considered outliers.

```{r constant_variance, fig.cap="Residuals vs. Fitted Values Plot for final model"}

# resvfittedval
high_res <- rstandard(final_model) %>%
  abs() > 2

ggplot(mapping = aes(final_model$fitted.values, final_model$residuals)) +
  geom_point(aes(color = high_res)) +
  geom_smooth(se = F, color = "red") +
  labs(x = "Fitted Values", y = "Residuals", color = "High Residual")
```

```{r normality, fig.cap="Q-Q Plot"}

# qqplot
ggplot(mapping = aes(sample = final_model$residuals)) +
  geom_qq() +
  geom_qq_line() +
  labs(x = "Theorhetical Quantiles", y = "Measured Quantiles")
```

<!-- I also had the caption issue with this -->
```{r outliers, fig.cap="Outliers"}

# get leverage from model
fm_lev <- hatvalues(final_model)

# print desired observations
filter(
  data,
  high_res,
  abs(fm_lev) > 2 * mean(fm_lev)
) %>%
  select(Name, PriceinUK) %>%
  knitr::kable(tabel.envir = "figure")
```

\newpage
# Code Appendix
```{r, ref.label=knitr::all_labels(), eval=FALSE, echo=TRUE}
```